 

# 微服务







# Java 基础

## 内存模型

本地方法区，程序计数器，栈，堆，方法区

其中99%的垃圾回收发生在堆和方法区

本地方法区是调用本地方法接口，其中多线程底层start方法，native 关键字就是调用本地方法接口

方法区（元数据）：存放static，final修饰的变量，class类模版，运行时常量池。

## new一个对象的过程

- 通过classload 类加载器class文件，应用反射生成class对象
- 通过class对象模版，实例化对象

## 类加载机制

JVM 使用双亲委派机制，其中类加载器有：BootstrapClassload加载器，ExtClassload加载器，appclassload加载器。

在加载一个class对象，类加载器会向上尝试还在对应类，一步一步向上检查是否加载，直到BootstrapClassload,如果在类加载器对应目录中未加载到对应类，会向下尝试加载。

双亲委派机制好处：能够保护核心类库安全，保证类的唯一性

双亲委派机制打破自定义类加载器，重新loadclass方法。其中tomcat就打破了双亲委派机制，tomcat是容器，保证多个web项目类隔离。

## 垃圾回收

垃圾回收主要发生在堆中，其中分为新生代和老年代，新生代又分为：eden区，幸存者from/to区。

新生代：新生代的垃圾回收称为轻GC，当eden区对象满之后，会触发轻GC，将还存在对象引用对象存入from1区，其余清除。等下次e den区对象再次满之后，轻GC会一起将eden,from区对象还存在对象引用对象存入to区，其余对象清除。经历过from/to区15次转换，还存活的对象会存入老年代。其中`eden:from:to =8:1:1`

在对象头中使用4个bit位来存储对象的年龄，最大为15.

![截屏2023-04-11 21.57.59](./img/面试/截屏2023-04-11 21.57.59.png)

老年代：发生的垃圾回收称为重GC



动态年龄判断：正常新生代from/to区15次转换之后还存活的对象会存入老年代，但是在还未到15次之前有可能超过空间的50%，就会需要存入老年代。动态年龄判断可以理解为：在到达x年龄时，内存使用空间超过50%，那么大于x年龄的对象就存入老年代。



老年代空间担保机制：在轻GC时，将年轻代的对象存入老年代，此时不确定要存入的对象能否在老年代存入。所以内部提供了老年代空间担保机制，在进行轻GC时，判断新生代对象总大小和老年代空间比较，判断是否进行重GC。

- 如果开启空间担保机制，在新生代总大小超过老年代空间时，会判断之前每一次进入老年代数据大小的平均值，如果超过就会进行重GC

- 如果在不开启，在新生代总大小超过老年代空间时，就会触发重GC。

空间担保机制可以避免频繁进行老年代的重GC，重GC比较消耗性能。



垃圾回收算法

标记清除算法：标记还存在对象应用对象，其余清除。容易产生内存碎片。

复制算法：将空间划分，将存活对象复制到新空间，然后清除其余数据。但是会浪费一部分空间。适用于对象存活时间短，类似于新生代中的对象。

标记整理算法：基于标记清除算法，然后将剩余对象进行整理，解决标记清除算法产生的内存碎片，但是整理是比较消耗性能。



- parnew垃圾回收器，是JVM年轻代垃圾回收器，是多线程执行，使用复制算法。

- CMS垃圾回收器：老年代回收器，此垃圾回收器是以吞吐量最优，要求最短垃圾回收停顿时间。使用标记清除算法，允许存在一定量的内存碎片，在超过碎片的阈值，会执行清除-整理算法。

其中CMS 存在4个阶段

1. 标记存活对象，会造成程序暂停（STW）,这个时间很快。
2. 并发标记：不会STW，链路追踪，垃圾回收线程和用户线程并行，标记对象
3. 重新标记：会STW，标记一些在并发标记阶段发生改变的对象
4. 清除：不会STW，和用户线程并发，清除已经死亡的对象

缺点：并发阶段还是比较消耗CPU性能，而且不能等老年代满了才进行，应为用户线程和垃圾回收是并行的，必须给垃圾回收期间并行的用户线程流出一部分空间，并且这些用户线程也会产生垃圾，是不会清除。

## synchronized

jdk 1.6之前是重量级锁，底层实现需要用户态和内核态的切换，比较耗费资源。1.6之后，对锁进行了优化，提出4个状态：无锁，偏向锁，轻量级锁，重量级锁状态，随着竞争激烈会进行锁升级。

使用方式：

- 修饰实例方法
- 修饰实例对象
- 修饰静态方法

四种状态主要依赖了java对象头中的Mark Word

- 无锁状态：锁标志位的值为：0 01。对象是没有做任何同步限制的。
- 偏向锁：偏向锁位、锁标志位的值为：1 01。偏向锁不会主动释放锁。会在mark word中将thread ID 设置为当前线程ID
- 轻量级锁：锁标志位为：00。在前两种状态情况，会复制mark word到栈帧中，进行cas交换，判断是否能加锁成功。***CAS失败的失败策略就是锁升级，不会自旋;自旋发生在重量级锁***
- 锁标志位为：10。可重入实现为检查_owner属性，如果该属性指向了本线程，_count属性+1，并继续执行代码。

![1da504f5eca747579baa12b0776a3a96](./img/面试/1da504f5eca747579baa12b0776a3a96.png)



# ES

是基于lunece实现，分布式的搜索引擎。

## 倒排索引

倒排索引数据结构

- Term index:词项索引
- Term dictionary:词项字典
- posting list：倒排表，对应匹配每个词项的数据ID，是一个int数组结构。其中内部使用了两种压缩算法，对于稀疏数组采用rbm算法，稠密数组采用for1算法

评分算法：BM25，TF_IDF两种评分算法。

搜索一次流程：将搜索词分词，查询每个词项命中的倒排表，统计每个倒排表中，每个数据id被命中的次数，进行排序。

词项字典底层使用的是前缀树数据结构



mapping：类比于mysql中的数据表结构，是JSON的数据结构。除了字段名称，类型之外，还会设置分词器，是否评分等。

keyword数据类型：说明此字段不进行分词，用于精确查询。text 数据字段是需要分词，生成倒排索引。



## bool-query

- Must  多个条件必须全部满足
- Should 多个条件其中部分满足 or关系，具体要满足几个，有特定的参数来设置should中要有几个条件满足
- Filter 和must 一样，但是会忽略评分

must 和should同时存在，should会失效。



Match 全文检索

Term 精确查询

区别是对match是要对搜索词进行分词，然后进行检索。

term 是搜索词直接进行精确匹配。



ES 中一般在生成索引时使用最细颗粒度分词，在搜索时使用最粗颗粒度分词。

# Spring

组成模块：Spring code，Spring context，Springaop,spring mvc, spring web

## Bean 生命周期

1. Bean 利用反射，初始化
2. 属性填充，此过程会有循环依赖问题，内部使用三层循环解决
3. 调用aware接口，invokeAwareMethord 接口，对Bean Factory 进行属性设置
4. 调用beanPostProcessor 前置处理器，用的比较的多的是applicationContextPostProcessor，完成对applicationContext设置
5. 调用init- mentored方法，这里要注意是否实现initializingBean 接口，如果实现执行afterPropertySet方法
6. 调用beanpostprocessor后置处理器，AOP是在此阶段完成，调用abstractAutoPoxyCreat
7. 使用get Bean 使用bean
8. 销毁流程

## 循环依赖问题

三级缓存：前两层数据结构为concurrentHashMap<String,Object>,第三层为：HashMap<string，ObjectFactory<>>

循环依赖过程（A,B类互相依赖）

- 实例化A类，进行属性填充，A是一个半成品。
- 为A类生成工厂类，ObjectFactory对象放入三级缓存
- 然后需要注入属性b，在1，2，3级缓冲查找，都未找到
- 实例化B类，进行属性填充，B是一个半成品。
- 为B类生成工厂类，Object Factory对象放入三级缓存
- 进行属性填充属性a，在1，2，3级缓冲中查找，在三级缓冲中找到A类的object factory对象，调用getobject方法，获取到真实的a对象（如果没有a类没有AOP，返回原是对象，否则返回代理对象。正常来说AOP操作是在初始化之后执行，这里要提前暴露），将a对象（半成品）放入二级缓存，并删除三级缓冲中a工厂类。
- 此时B类完成属性填充，将B放入一级缓存，此时b是一个完整对象。
- 然后填充A类中的b属性填充，从一级缓存中获取到完整的b对象，进行填充。
- 完成A类的属性填充，将A放入一级缓冲，删除二级缓冲中的A的半成品。



## Bean作用域

singleton作用域，单例的。所有线程使用用一个Bean，可能存在线程安全问题（区分有状态bean和无状态Bean）。

prototype作用域，每个线程获取bean的时候，都会新创建一个新的Bean,不会存在线程安全问题。

还有一些web的作用域 request，session



有状态Bean：有实例变量的Bean,用户数据存储，是线程不安全的。

无状态Bean：不做数据保存，不变类，类似dao类



## Spring事务

事务和数据库保持一致，spring是关闭了数据库自动提交，将提交回滚交给spring管理。

事务不生效情况

- private 修饰的方法，JDK动态代理使用接口，私有方法一般不写在接口中，CGlib动态代理使用继承实现，私有方法不能被继承。
- Rollbackfor异常捕获有误，如果定义为RuntimeExeception,但是内部抛出I/O异常，这样是捕获不到的。
- 事务期间，做了线程切换，也会造成事务失效，事务实现过程中变量是定义在ThreadLocal 上的。

事务代理对象产生的过程

- 调用构造方法进行实例化

- 调用对象set方法进行属性填充

- 调用后置处理器的前置方法

- 调用invokInitMehtod方法初始化

- 调用后置处理器后置方法，其中调用abstractAuthProxyCreator中的after方法，来判断原始对象是否能找到合适的增强器

  通过切入点，方法判断器，判断类，方法上是否有事务注解。如果方法有事务注解，读取注解基本属性信息（事务传播机制等）

- 找到合适的增强器，调用create Proxy方法创建代理对象，具体使用哪种动态代理，依据是否实现了接口决定。

- 创建代理对象之后，放入一级缓冲中。

事务传播机制

事务传播机制存在其中，其中`required`,`requires_new`,`nested`常用。

 (标记位 `newthransation=true`才会提交事务，标记位是标记每个事务的)

- required(外部没有事务就创建，如果有就加入到事务中)
  - 外部事务开始，将数据库自动提交关闭
  - 拿到数据库连接并放入threadLocal 中，并且外部事务标记为`newthransation=true`
  - 内部事务开始从threadLocal中获取数据库连接，并且内部事务标记`newthransation=false`
  - 执行完逻辑代码，执行到外部事务，取到数据库连接，并且`newthransation=true`
  - 然后执行commit操作，移除threadLocal中的数据库连接，恢复自动提交功能
- requires_new（外部没有事务就创建，如果有就挂起，创建新事务）
  - 外部事务开始，将数据库自动提交关闭
  - 拿到数据库连接并放入threadLocal_A 中，并且标记为`newthransation=true`
  - 内部事务从threadLocal_A取数据库连接，如果取到，就存入threadLocal_B中，清除threadLocal_A中的数据库连接，实现外部事务挂起。
  - 创建新的数据库连接存入threadLocal_A中，内部事务的`newthransation=true`
  - 内部事务执行完，会提交事务，移除threadLocal_A中的新数据库连接，恢复自动提交
  - 然后判断是否有挂起的线程，如果有就从threadLocal_B移除存入threadLocal_A，并且标记位`newthransation=true`
  - 然后执行commit操作，移除threadLocal中的数据库连接，恢复自动提交功能
- nested(外部事务会创建回滚点)
  - 外部事务开始，将数据库自动提交关闭
  - 将拿到的数据库连接放入threadlocalA中去。 并且设置标记位newThransaion=true
  - 内部事务再次从threadlocala取连接，能取到数据库连接，标记位newThransaion=false
  - 此时会创建一个回滚点
  - 内部事务执行完业务代码，如果异常，回滚回滚点，如果正常，清除回滚点 



required:外层如果没有就会新建一个事务，如果有就加入到。

requires_new:不管有没有都会新建事务，如果外层有，会挂起外部事务，内外互补影响，内存回滚不会影响外层。

support：外层有就加入，没有就执行非事务方法。

Not_support:不管有没有都不执行事务，外层有的话，就挂起，执行非事务

mandatory：外层没有就抛出异常，如果有就加入

never：没有就执行非事务，如果有就抛出异常

nested：如果没有就新建事务，如果有就嵌套事务，外层回滚，会回滚内外，内层回滚，只回滚内。



## Spring AOP

正常程序开发是面向对象开发，代码自上而下的执行。这样会产生一些横切性问题，例如日志记录，性能检测，事务等。

这些任务和主业务没关系，将这些任务模块化，使用解耦，以动态代理的方式解决横切性问题，这样只需要关注模块化代码的执行逻辑，执行时机，执行位置。

- 目标对象(target)：使用的AOP的对象。

- 连接点(join-point)：目标对象中的所有方法。
- 切点(point cut)：一部分连接点，要增强的目标方法
- 通知：定义了公共逻辑的执行时机，和执行逻辑，其中环绕通知可以拿到参数，在逻辑内部调用目标对象的原方法。
- 切面：切点和通知组成了完整的一个切面。

AOP底层实现使用了JDK动态代理，CGlib动态代理。

- JDK动态代理：使用反射实现，主要使用了反射包中的Method，Proxy，InvocationHandler对象。

  - Method：目标对象中的方法
  - Proxy：创建代理对象
  - InvercationHandler：重写invoke方法，其中实现公共逻辑

  JDK动态代理是实现目标对象的接口，继承proxy类。会生成一个字节码文件。使用反射机制回调。使用JDK动态代理目标对象必须实现了接口

- CGlib动态代理：使用ASM1操作字节码生成代理对象，主要使用Enhancer 类来创建代理对象。

  会生成三个字节码文件，CGlib动态代理是继承了目标对象类，实现了Factory接口，重写目标对象的中所有方法。因为使用继承实现的，被代理对象和方法不能使用final关键字修饰。

# HashMap

默认参数：扩容因子：0.75，默认数组大小：16

## 版本差异

- 1.7:数组+链表数据结构，插入数据时，采用头插入方式插入，头插入在扩容多线程情况下，会产生环形链表造成死循环。（无限进行链表反转）
- 1.8:数组+链表+红黑树数据结构，采用尾插入的方式。数组初始化是在put方法中，懒汉模式。



## hash 方法

```java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

采用hashcode右移16位异或计算

![截屏2023-03-17 20.00.19](./img/面试/截屏2023-03-17 20.00.19.png)



- 将hashcode高位16位右移16位，高位以0补充
- 和原hashcode 进行异或运算，计算出新的hash值

一般数组长度比较小，使用hash值计算数组index时，只有低位参与了散列计算。在hash方法中，将高位进行右移16位，高位和低位进行运算结果，同时保存了高位和低位的信息，这样在计算index1时，高位和低位都参与了计算。

这样hash散列度更高，减少hash冲突。

## 计算index

```java
n = (tab = resize()).length;
if ((p = tab[i = (n - 1) & hash]) == null)
```

使用hash方法获取的散列值和数组长度-1进行位运算获取index1值。

- 使用散列值，可以在数组中分布均匀
- 使用数组长度计算，可以使index1值不会超出数组长度
- 这里使用位运算，效率比取模运算效率高

![截屏2023-03-17 20.28.37](./img/面试/截屏2023-03-17 20.28.37.png)

这里也解释了每次扩容为什么长度是2的幂次方，长度2的幂次方，length-1对应二进制的值`....1111`在进行，这样在进行计算之后分布相对均匀，这样能提升查询效率，空间也不浪费。



## put方法 

- 通过hash方法获取到hash值
- 判断table 是否为空，如果为空会发生初始化table。（JDK1.8使用懒汉模式）
- 通过`(n - 1) & hash`计算出数组index，判断对应位置是否有值
- 如果为null，将key,value封装称为node对象存到对应位置（若桶为空，就会出现覆盖情况，在多线程情况下是线程不安全的）
- 如果不为null,发生hash碰撞，判断hash值和key值是否和原位置值一致，如果相等，新的value替换原位置的value。
- 如果和原位置的hash,key不想等，则需要储存新的对象。
- 判断原位置对象是否为`treeNode`对象（继承了node对象）。如果是树对象，将数据封装为树对象，存在红黑树中。
- 如果不是树对象，封装为`node`对象，使用尾插入法，插入到链表中。这里需要判断链表长度和数组长度。
- 判断如果链表长度小于8，插入到链表中。
- 判断如果链表长度大于8，并且数组长度大于64的时候，会转化为红黑树。
- 如果如果链表长度大于8，并且数组长度小于64的时候，会先进行扩容，然后插入。



## get方法

- 通过hash方法获取到hash值
- 通过`(n - 1) & hash`计算出数组index，判断对应位置是否有值
- 如果为null，返回null。
- 如果不为null,取第一个节点，判断hash值和key值是否和第一个节点值一致，如果相等，返回第一个节点。
- 如果和原位置的hash,key不想等
- 判断下一个节点是否为`treeNode`对象（继承了node对象）从红黑树中取。
- 如果不是树对象，遍历链表，直到取到值。

​	

## 扩容机制

hashmap中数组最大长度为2^30。在如下情况会发生扩容。

- 在map中存在的元素超过阈值时 
- 在添加元素，链表长度超过8，数组长度小于64时

扩容步骤

- 判断原数组是否为空数组，如果是进行初始化。
  - 无参构造函数，设置长度为默认16，阈值为12（扩容因子0.75*16）
  - 有参构造函数，传入数字a，取大于a的，最小2的幂次方。
- 如果不是初始化，判断原数组长度是否达到最大长度2^30,如果没有原数组长度*2。
- 计算出新数组长度的阈值
- 遍历数组，取第一个`node`节点，如果不为null,判断是否为链表，单node，红黑树
  - 单node，`hash & (newCap - 1)`计算index方法，计算新数组中位置
  - 遍历红黑树，获取新数组位置
  - 遍历链表，判断每个节点是高位还是低位。如果是低位（0）还是原数组index，如果是高位（1）是原数组index+原数组长度

![截屏2023-03-18 22.39.10](./img/面试/截屏2023-03-18 22.39.10.png)



# JUC 并发包

## ConcurrentHashMap

- 1.7：分段锁，默认并发是16，一旦初始化，Segment 数组大小就固定，后面不能扩容

- 1.8：Volatile+CAS + synchronized 来保证并发安全性

  ```java
      static final int MOVED     = -1; // 代表正在扩容,get 获取数据时，从新数组获取 
      static final int TREEBIN   = -2; // 树节点标识
      static final int RESERVED  = -3; // 预定占位
  ```

### put 方法

- 判断key,value都不允许为Null
- 计算key的hash值
- 进入死循环，判断数组table是否为空，如果为空，执行initTable方法
- 通过`(n - 1) & hash`计算出数组index，判断对应位置是否有值；如果当前位置没有数据，采用CAS方式插入数据，插入成功返回true，跳出循环；
- 判断当前位置的Hash值是否为`MOVED`，如果为`-1`,表示正在扩容，`helpTransfer`去协助扩容迁移数据
- 如果不是以上情况，说明发生了hash冲突，需要把数据添加到链表或者红黑树上
- 使用synvchrinized 锁住当前桶的位置，拿到当前位置数据判断和锁是否同一个。
- 取当前位置的hash值判断是否大于等于0 ，如果是当前桶下是链表或者为空。（hash值为0到n-1,红黑树的hash为-2，hash值大于等于0，可能是链表，也可能是null）
  - 如果是链表：与原位置判断hash,key，来决定是更新数据，还是把数据加到链表尾部
  - 如果是红黑树：使用红黑树
- 判断链表中的个数是否是否大于8，如果大于等于8，调用`treeifyBin`判断尝试将链表转化为红黑树,或者尝试扩容
- `addCount`添加元素个数，在方法内部，如果超过阈值，会进行扩容`transfer`

### initTable 方法

>sizeCtl
>
>- -1表示数组正在进行初始化
>- 小于-1：表示正在扩容
>- 0：表示还没有初始化
>- 正数：如果没有初始化，代表初始化的数组长度。如果已经初始化，代表下次扩容的阈值



- 判断数组是否初始化，如果还没初始化走之后逻辑
- sizeCtl 复制给sc,判断sc是否小于0，如果小于，线程礼让CPU资源
- CAS修改sizeCtl的值为`-1` 如果能修改成功，说明可以进行初始化
- 再次判断数组是否为空
- 获取数组的初始化长度，如果sc大于0，使用sc作为长度，如果为0，使用默认16
- 初始化数组，然后修改sc的值，得到下次扩容的阈值

### treeifyBin 方法

- 判断数组长度是否小于64，如果小于`tryPresize`尝试扩容
- 否者则进行链表转为红黑树

### tryPresize方法

- 对扩容的数组长度做判断，判断是否达到数组最大长度，如果未达到最大长度，保证数组长度是2的n次幂
- 拿到`sizeCtl`判断是否大于0，正数：如果没有初始化，代表初始化的数组长度。如果已经初始化，代表下次扩容的阈值
- 判断数组长度是否为空，如果为空执行初始化数组（与initTable 一致）
- 判断扩容长度是否小于扩容阈值，如果小于说明扩容已经完成
- 不是以上情况，进行扩容操作。判断sc小于0，说明已经开始扩容，帮助扩容`transfer`

### helpTransfer 方法

- tab不为空，且节点为Move类型，且节点下一个元素不为空。（为空则说明是空节点，直接返回）
- 调用`resizeStamp` ,高16位代表扩容的标记（和数组长度有关）、低16位代表并行扩容的线程数
- 当处于迁移状态时（即sizeCtl<0则说明处于迁移态），自旋
  - 扩容容量改变，或扩容结束，或辅助扩容线程达到最大限制，或扩容任务已经分配完，则退出。
  - 修改sizeCtl线程数+1，并辅助迁移数据。辅助完则退出。

## ReentrantLock

实现了`lock`接口

- 内部维护了`sync`抽象类，`sync`继承了`AbstractQueuedSynchronizer`
- 公平和非公平分别对`sync`进行实现`NonfairSync`,`FairSync`

![image-20230418103417224](./img/面试/image-20230418103417224.png)

`AQS`为加锁和解锁过程

获取锁失败的线程，会进入`CLH`队列阻塞，其他线程解锁会唤醒`CLH`队列线程。

释放资源之后，会唤醒`CLH`队列阻塞的线程，重新竞争锁。

![image-20230418112443653](./img/面试/image-20230418112443653.png)

此时可能还有非`CLH`队列的线程参与竞争，所以非公平就体现在这里，非`CLH`队列线程与`CLH`队列线程竞争，各凭本事，不会因为你是`CLH`队列的线程，排了很久的队，就把锁让给你



公平锁和非公平锁

1. 非公平锁在`lock`方法中获取锁资源时会首先通过CAS进行获取锁资源，竞争失败才会调用`acquire`方法。公平锁中直接调用`acquire`方法。

   `acquire`方法定义在父类AQS中，首先是调用`tryAcquire`方法尝试获取锁，失败则调用`addWaiter`方法将线程包装为Node。然后自旋CAS入队，接着在`acquireQueued`方法中完成线程的阻塞：

   ![image-20230418144038993](./img/面试/image-20230418144038993.png)

   

2. 在`tryAcquire`方法中，非公平锁在获取锁资源会判断<font color=red>判断队列中是否有优先于此线程的排队线程</font>

   

   ![image-20230418113142338](./img/面试/image-20230418113142338.png)

# Redis

单线程快的原因

- 数据操作是在内存中操作
- 使用单线程，没有线程切换的消耗
- I/O依赖epoll的多路复用机制，提升了I/O的利用率
- 高效的数据结构，全局hash表和高效的数据结构

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

对于过期的key的处理机制

- 惰性删除：过期之后不会马上删除，在每次读写一个过期key的时候，先判断过期key是否过期，如果过期就删除
- 定时删除：定时会删除一部分已经过期的key
- 定期删除：定期采样一部分key，删除其中过期的key;如果过期key占比超过可接受的百分比，继续删除操作

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 主动清洗策略

如果超过内存最大设置，会触发主动清洗策略

- 不做清除，拒绝写入操作
- 对于设置过期时间的key
  - 随机删除
  - LRU算法删除，最近最少使用的key
  - LFU算法删除，最近最不经常使用的key
- 对于全部的key
  - 随机删除
  - LRU算法删除，最近最少使用的key
  - LFU算法删除，最近最不经常使用的key

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 缓存穿透，缓存击穿，缓存雪崩

- 缓存穿透：缓存中查不到数据，数据库也查不到数据
  - 参数合法性验证
  - 数据库查不到数据也写入到缓存中，设置过期短一点
  - 布隆过滤器
- 缓存击穿：缓存中查不到数据，数据库能查到，一般发生在初始化或者key过期的情况，在高并发场景下，大量请求同时请求数据库（针对热点数据）
  - 热点数据永不过期
  - 请求数据库操作加锁，防止并发
- 缓存雪崩：缓存大面积过期，导致大量请求数据库（针对大量的key）
  - 设置key的过期时间加一个随机数

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 分布式锁

- 使用set-nx,set-ex两个命令实现；setnx 如果不存在，则设置成功（表示拿到锁）setex设置过期时间
- 上面两个命令是两步操作，不是原子操作，可使用`set（key,value,nx,px）`命令实现
- 使用lua脚本



1. 业务时间大于设置的超时时间。使用redisson 看门狗机制，实现过期时间续期

2. 加锁，释放锁不在一个同一个线程，错误删除其他线程锁（锁中加对应线程ID，删除时进行判断，lua脚本）
3. 主从异步复制，造成锁丢失。（使用红锁来解决，向5个节点请求，根据超时时间来判断是否跳过锁，3个节点能拿到锁并且花费时间小于有效期，才算拿到锁 ）

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 主从复制原理

- 全量复制：主节点通过bgsave命令fork出子进程进行rdb持久化，主节点通过网络将rdb文件传入从节点，从节点清空数据，载入新的rdb文件，此过程会阻塞，无法相应客户端命令。
- 增量复制：复制偏移量和缓存区。主从节点都会维护一个偏移量
  - 复制积压缓冲区，主节点会维护一个固定一个，先进先出队列做为复制缓冲区，当主从节点offset超过缓冲区，只能执行全量复制
  - 服务器运行ID，从节点判断主节点id和从节点保存的主节点是否一致。如果一致根据偏移量和缓冲区判断是否全量复制，不一致进行全量复制。

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 集群选举机制

1. 当从节点发现主节点变为Fail
2. 将自己记录的currentEpoch加1，并且广播发起重新选举（FAILOVER_AUTH_REQUEST）
3. 其他节点收到广播信息，只master响应，判断请求者的合法性，并响应，对每一个epoch只发送一次ack
4. 尝试选举的从节点收集（FAILOVER_AUTH_ACK）
5. 收集到超过半数的主节点响应，会变成主节点
6. 然后通知其他集群节点

从节点不是在主节点一进入fail状态就发起选举，会有一定时间的延迟，并且每个从节点延迟时间不同。

- 不立即选举，是确保主节点fail信息在其他节点传播，防止其他主节点未意识到fail状态拒绝投票。

- 每个从节点延迟时间和从节点复制主节点的数据量rank,rank越小代表数据越新，理论上选举持有最新数据的从节点首次发起选举。

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 扩容机制

Redis内部维护两个全局hash表（x,y），先使用x表，Y不会分配内存空间。随着HashX的数据增多，会有多个key存入hash table的同一个槽位中，这样就会产生Hash冲突。产生冲突并不是马上进行扩容rehash，而是每个槽位都使用一个链表存储key，数据量的增大，链表会越来越长。

![image-20230328214532797](./img/面试/image-20230328214532797.png)

Hash桶的每个元素记录的是entry中的key和value的指向地址，每个Hash桶及其连续的数组占用的内存空间和存储的数据大小并无关系。

负载因子: loader_factor：`哈希表中键值对数量 / 哈希表长度`



扩容发生条件

- 当没有子线程的时候（数据持久化的时候，会fork出子线程），并且负载因子大等于1
- 当有子线程的时候，并且负载因子大等于5

Rehash操作

Hash表Y申请较大的空间，如hash桶数组容量是hash表X的两倍。将hash表X中的元素重新计算hash和取模运算，映射到hash表Y中。

清理hash表X。

以redis存储数据的数量级,并不是一次性的完成整个数据的重新copy。（这样会导致主线程阻塞）

<font color=red>渐进式 rehash</font> 

在Rehash开始时，两个Hash表维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。

在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会将HashX上的键值对Rehash到HashY上面，并将rehashidx+1.

随着不断的操作，直至所有的节点数据copy完成；将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。



渐进式 rehash 的过程中,同时使用HashX,HashY两个表，删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行，查找一个键的话， 程序会先在 X里面进行查找， 如果没找到的话， 就会继续到 Y里面进行查找。如果新增键值对，一律会被保存到 Y里面， 而 X则不再进行任何添加操作.

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

### 持久化写时复制

首先在redis进行持久化的时候，执行bgsave或者bgrewriteaof命令就会fork出一个子进程读取数据，从而写到磁盘中。

Fork时并不会一次性拷贝所有内存的数据给到子进程，fork采用操作系统的写时复制（copy-on-write)机制，就是为了避免一次性拷贝大量备份数据给子进程造成的长时间阻塞。但是fork子进程需要拷贝进程必要的数据结构，比如内存页表（虚拟内存和物理内存的映射索引表）.拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小

fork()之后，kernel把父进程中所有的内存页的权限都设为read-only，然后子进程的地址空间指向父进程。当父子进程都只读内存时，相安无事。当其中某个进程写内存时，CPU硬件检测到内存页是read-only的，于是触发页异常中断（page-fault），陷入kernel的一个中断例程。中断例程中，kernel就会把触发的异常的页复制一份，于是父子进程各自持有独立的一份

Rehash阶段，写操作是无法避免的。所以Redis在fork出子进程之后，负载因子阈值会提高，减少扩容和ReHash.减少不必要的内存写入操作。

![image-20230328221132699](./img/面试/image-20230328221132699.png)

### 持久化机制

- RDB 快照，定时生成快照文件
- AOF ：记录每个操纵
- 混合持久化：注意开启混合持久化的前提，必须开启AOF持久化

> AOF在重写(aof文件里可能有太多没用指令，所以aof会定期根据内存的最新数据生成aof文件)时将重写这一刻之前的内存指令，生成rdb快照文件。重写过程中，新的改变数据集的命令，会暂存在内存中，重写结束之后，追加到AOF之后。这两种格式数据都写入新的aof文件，新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，原子的覆盖原有的AOF文件，完成新旧两个AOF文件的替换；
> AOF根据配置规则在后台自动重写，也可以人为执行命令bgrewriteaof重写AOF。 于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。
> ————————————————
> 版权声明：本文为CSDN博主「Abner G」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/Abner_G/article/details/123466185



# Mysql

## B树和B+树的区别

![截屏2023-04-06 22.04.06](./img/面试/截屏2023-04-06 22.04.06.png)

- 一个节点存在多个元素
- 数据是进行了排序
- 叶子节点之间存在双向指针
- 非叶子节点的元素会在叶子节点冗余一份（非叶子可以存更多数据，叶子节点可以存放更多数据）

## 回表

回表：在一次查询中查询字段包含非索引列，一次索引查询查询不到所有列信息。需要到表中再次查询相应列信息。这个过程叫做回表。

是否回表，取决于查询条件是否为主键索引。 主键索引，是聚簇索引B+树的叶子节点存储列该行所有数据；普通索引叶子节点存储只有该索引和主键id。

select * from（id为主键）如果查询条件为id=2.此查询只需要id的索引树 不需要回表。如果查询条件为name=’xx’.查询过程为先查询name索引树，查到对应主键id，然后再根据id再查询一次id索引树，先定位主键，再定位行。



## 为什么sql会用错索引

索引的选择在MYSQL中是由优化器完成。

优化器选择机制为：选择最优索引执行，而扫描行数是影响查询的重要因素之一，行数越少，磁盘IO越少，查询越快。 （是否使用临时表，是否排序）也会影响优化器选择。执行之前，扫描行数为一个估算值。此估算值就和索引有关系，为索引区分度（基数）  ；会在n个数据页中，获取不同值个数，然后取平均数，然后乘索引页数。得到索引基数。

数据在持续更新过程中，这个基数也是持续改变 因此可能导致用错索引。



## 联合索引最左原则

只能从最左边字段开始查询，并且不能跳过（最左）字段；否则不走索引。

本质：只有从最左列开始才会走索引

查询过程：根据最左边的字段在索引树查找符合条件的数据，然后在上一步的查询基础上根据其余字段做匹配查询



## 索引创建原则

- 经常作为查询条件，排序，分组字段

- 索引列区分度要足够高

- 避免给大字段创建索引

- 频繁增删改的字段不创建



## 索引失效的场景

例（表中，a为主键，b，c，d为联合索引）

- 联合索引未遵循最左前缀原则，过滤条件中未包含联合索引的最左列b
- 范围查找索引失效，当查询字段是b>5这样的条件，要看查询字段是否需要回表，如果查询字段为联合索引的字段或者主键字段，是不需要回表的；反之则需要回表，导致索引失效
- Order by 排序导致索引失效，取决于是否查询字段是否需要回表，如果查询的为全部字段，MYSQL排序的执行流程为，全表扫描，然后排序，不会使用索引。
- 对数据类型转换造成索引失效，（MYSQL中对非数字的字符串转换为0，数字字符串为对应数字）假如b字段为varchar 类型，过滤条件为b=1,需要全表扫描将b字段进行转换，然后进行比较



## mysql中join的理解

小表驱动大表即：小表数据集驱动大表的数据集

join查询过程中，驱动表A是全表扫描，被驱动表B是树搜索。

执行流程：从A表读取一条数据，取关联字段，去B表查询符合条件的数据，和A表结果组成一行。重复之前步骤，直到A表循环结束。



>大厂推荐在应用层关联
>
>基于大厂业务，业务数据等因素。在join操作效率会不能保证。
>
>在应用层做的话可以减少锁的竞争，数据库更容拆分，高可用。可以减少冗余数据的查询。



## InnoDB &MyISAM区别

- InnoDB 支持事务，MyISAM不支持事务

- InnoDB 支持外键，MyISAM不支持

- InnoDB 引擎索引实现聚集，底层为两个文件来看，（索引和数据存在一个文件，表结构等信息存在一个文件中），数据结构来看，叶子节点中存放的是主键和所有字段数据；MyISAM中索引是非聚集的，底层文件来看为（表结构等信息存在一个文件中，索引在一个文件中，数据在一个文件中），数据结构来看，叶子节点存放的是数据文件指针

- InnoDB 不单独保存表的行数，MyISAM 会单独保存表的行数

- InnoDB 最小颗粒度为行锁，MyISAM 最小颗粒度为表锁，在执行更新操作，MyISAM 会阻塞其他线程，并发不优秀。



## 事务四大特性保证依据

Innodb引擎级别日志undo.log,redo.log。MYSQL server 级别有bin.log

- 原子性是undo.log保证，记录执行数据SQL，主从同步也是使用undo.lo实现

- 一致性是其他特性和程序共同维护的

- 隔离性是MVCC保证

- 持久性是redo.log保证，在事务提交之后，会在redo.log中写一个commit记录



## 事务隔离级别

事务隔离机制是为列解决：脏读，不可重复读，幻读问题。

脏读：一个事务读取到了其他事务未提交的数据

不可重复读：在一个事务范围内多次查询某条数据，返回结果不同。是因为在查询间隙数据被其他事务修改提交。

幻读：事务在读取某个范围内数据时，另外事务在该范围内插入新数据（插入数据之前查询ID是否存在，在插入的时候出现主键ID冲突），前置查询无法满足后续需要。就是一个事务前后两次读取到的数据条数不一致。

>脏读和不可重复读区别在于：脏读读取到列其他事务未提交的数据，不可重复读是读取了其他事务已提交的数据。
>
>不可重复读与幻读都是读到其他事务已提交的数据,但是它们针对点不同.
>
>不可重复读:update.
>
>幻读:delete,insert.



- 读未提交：所有事务都能读取到其他事务未提交的结果。

- 读已提交：一个事务只能看到其他事务已提交的结果，解决脏读问题

- 可重复读：mysql默认隔离级别，统一事务读取数据时，获取的相同的数据行。解决不可重复读问题

- 串行化：事务进行排序，不能相互冲突。



可重复读解决了部分幻读现象，不能解决所有幻读问题

快照读（普通 select 语句）：使用MVCC隔离，可以减少加锁带来的开销（select * from table）

当前读（select … for update 等语句）：是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select … for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。

 

在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

因为这种特殊现象的存在，所以我们认为 MySQL Innodb 中的 MVCC 并不能完全避免幻读现象。



![image-20230406224511072](./img/面试/image-20230406224511072.png)

生产环境大多使用读已提交隔离级别RC，可重复读隔离级别RR存在间隙锁，增加了出现死锁的几率。

RR级别下，条件未命中的索引会锁表，RC环境只是锁行。

RC并发优秀。



## MYSQL MVCC

MVCC 多版本并发控制，依靠undo.log，版本链（隐式字段），read view实现。 

当前读：是通过加锁（排他锁和共享锁）来实现。

快照读：是不加锁的非阻塞读，是基于MVCC实现的，维护了数据的多个版本。（每个版本对应一个快照）

 

每条数据都有隐式字段：TRX_ID（最近修改（修改、插入）事务ID），ROLL_PTR回滚指针

TRX_ID是单向递增 

Read view

决定读取数据时，读取数据的哪个版本。

Read view 中有3个参数：trx_list(当前未提交事务ID)，up_limit_id（当前未提交事务ID中最小id）,low_limit_id（Read View 生成时刻系统尚未分配的下一个事务ID，也就是等于目前出现过的最大事务ID + 1）

当事务读取到数据的某一版本时，会拿当前版本的事务id和read view 中的属性进行可见性算法计算，然后决定读取哪个版本数据。

读已提交：是在select时生成read view

可重复读：是事务级别的，在创建事务的时候，创建read view

## MYSQL中的锁

按操作类型分类

读锁/共享锁：对资源加读锁，其他事务可以加读锁，不能加写锁

写锁/排他锁：对资源加写锁，其他事务不能加任务锁

 

按锁颗粒度分类

全局锁：对整个数据库加锁

表级锁：分为表锁和元数据锁，元数据锁是在改变表结构的时候，自动加

行级锁：是InnoDB中支持，

页级锁：是BDB引擎中支持

 

更新数据时未使用索引，会上升为表级锁。

更新数据时使用索引会使用行级锁

Select * from for update 会加行级锁，同时会加间隙锁

 

行锁中分为记录锁和间隙锁，Next-key lock 

记录锁：主键索引，唯一索引加速

间隙锁：二级索引，锁住间隙，防止幻读。

Next-key lock 记录锁和间隙锁的组合



## Mysql 中in语句查询

Mysql中in语句查询会转化为范围查询，例如in(a,b),会转化为两个[a,a]，[b,b]范围查询

![image-20230406225300194](./img/面试/image-20230406225300194.png)

## 分页查询优化

分页查询使用“select c1,c2,… from t1 limit n,m”语句，工作原理就是先读取前面n条记录，然后抛弃前n条，读后面m条想要的，所以n越大，偏移量越大，性能就越差。（这种查询机制每次都是从数据库第一条开始扫描）

- 优化（方向是减少查询得扫描行）

通过索引进行分页，`limit`机制是从结果集的N位置，取M条数据。如果存在`id`自增主键，可以先使用分页参数计算出`id`值得范围，然后取对应结果集<font color=red> （id字段必须是连续）</font>

```sql
select * from sbtest1 where id in (select id from sbtest2 where k=504878) limit 100;
```











# clickhouse

## Join操作

clickhouse 的join操作分为：单机&分布式join。算法分为`HashJoin`,`MergeJoin`.

1. 单机join

`HashJoin`:一般情况下最高效的 Join 实现，所以 Clickhouse 默认选择这种方式。哈希连接的执行过程包括建立阶段（Build Phase）和探测阶段（Probe Phase）。

- 建立阶段：选择一张表，对 Join Key 做哈希建立 Hash Map，全部放在内存里。Clickhouse 会一直以右表来建立Hash Map。
- 探测阶段：另一个表作为探测表，扫描它的每一行并计算 Join Key 的哈希值，在 Hash Map 中试探是否可以 Get（key）。考虑到哈希碰撞，若哈希值相等，需要再判断一次原始 Key 值是否相等，返回最终满足条件的记录。

`MergeJoin`:可以分为排序和归并两个阶段

- 排序：就是对两表根据 Join Key 进行排序
- 归并：因为两张表已经按照同样的顺序排列，两表各一次循环遍历即可。

A、B 两个表各设置一个指针，分别为 I 和 J，若A表第 I 行 Key 小于 B 表第 J 行 Key，则 I ++，否则 J ++，直到匹配到数据则添加到结果集。若表的记录本身就是有序的，或者 Join key 刚好是索引列，选择排序归并成本更低。

2. 分布式join

Clickhouse 的分布式 Join 查询可以分为两类，不带 Global 关键字的普通 Join，和带 Global 关键字分布式全局 Join。



![截屏2023-04-11 21.37.32](./img/面试/截屏2023-04-11 21.37.32.png)











# RabbitMQ

![image-20230321092839790](./img/面试/image-20230321092839790.png)

基于AMQP机制实现的消息中间件，整体上是生产者与消费者消息模型，实现了消息接收，存储，转发功能。

### 工作流程

- 生产者将消息（消息头中存储了RoutingKey）发送到RabbitMQ上的Exchange上

- Exchange交换机根据路由规则将消息分发给不同的队列
- 最后再把消息传递给订阅了这个队列的消费者



Exchange交换机定义消息路由规则

### 基本概念

- channel 信道：消息推送使用的通道
- message 消息：消息头和消息体组成
- routingKey 路由键：消息头中的一个属性，和bindingKey 一同决定消息路由。一般都是有一个或多个单词组成，多个单词之间以"."分割，例如： item.insert
- Exchange 交换机：定义生产者和队列的匹配规则
- queue 队列：用于存储消息，并且队列中的消息被一个消费者消费之后，不能被其他消费者消费。
- bindingKey 绑定键：定义交换机和队列的绑定规则。



### 消息路由

RabbitMQ路由策略是由Exchange类型和binding决定。每个交换机更具binding在内存存在一个交换机和队列的路由关系表。生产者在发送消息时，在消息头中发送一个routingKey，交换机拿到routingkey之后取路由关系表中匹配bindingKey,匹配规则则是由交换机类型决定的。

常用交换机类型：fanout,direct,topic

### 消息模型

- Fanout 广播模型：生成者发送消息到交换机，交换机将消息转发到所有绑定的队列（不做routingkey和bindingkey的匹配），这样每个消费者都能拿到消息，实现一个消息多个消费者消费。

  - 可以有多个消费者
  - 每个消费者绑定自己的队列
  - 队列都绑定到交换机上

  ![image-20230321103922880](./img/面试/image-20230321103922880.png)

- Direct 定向模式：队列只订阅一部分消息。队列和交换机绑定需要指定一个BindingKey,生产者在向交换机中发送消息时，要指定消息的routingKey.需要BindingKey和routingKey完全匹配

  - P：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。
  - X：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列
  - C1：消费者，其所在队列指定了需要routing key 为 error 的消息
  - C2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息

  ![image-20230321105012604](./img/面试/image-20230321105012604.png)

- Topic 主题模式：类似于Direct模式，只是BindingKey和routingKey的匹配规则使用通配符。

  - 通配符匹配规则：`#`匹配一个或多个词,`*`匹配不多不少恰好1个词

  ![image-20230321105527404](./img/面试/image-20230321105527404.png)

- Sample 简单模型：生产者将消息发送到队列，消费者从队列中获取消息，队列是存储消息的缓冲区。

  ![image-20230321110110123](./img/面试/image-20230321110110123.png)

- work 工作模型：一个工作队列，在多个工作者之间分配耗时任务(平均分发),两个消费者一起消费，两个消费者各自消费了一半的消息，而且各不相同，这就实现了任务的分发。

  - 平均分发存在问题，在有的消费者耗时比较长，那么消费快的消费者就会有大量闲置时间
  - 配置实现`能者多劳`，使用basicQos方法和prefetchCount=1 设置。通知RabbitMQ一次不要向消费者发送多于一条消息。（不要向消费者发送新消息，直到它处理并确认了前一个消息。这样可以将消息分派给不忙碌的消费者）

  ![image-20230321110313442](./img/面试/image-20230321110313442.png)

### 消息确认机制

生产者确认机制

- 事务机制，是基于AMQP实现

  ```java
  channel.txSelect(); // 将当前channel设置成事务模式
  
  /**
    * ConfirmConfig.exchangeName(交换机名称)
    * ConfirmConfig.routingKey(路由键)
    * message （消息内容）
    */
  channel.basicPublish(ConfirmConfig.exchangeName, ConfirmConfig.routingKey, MessageProperties.PERSISTENT_TEXT_PLAIN, message));
  channel.txCommit(); // 提交事务
  channel.txRollback(); // 回滚事务
  ```

  解决producer与broker之间消息确认的问题。但是会产生生产者和MQ之间的等待确认，降低了MQ的 性能。

- confirm 机制

  生产者将信道设置成confirm模式，一旦信道进入confirm模式，所有在该信道上面发布的消息都会被指派一个唯一的ID(从1开始，单调递增)，一旦消息被投递到所有匹配的队列之后，broker就会发送一个ACK给生产者（包含消息的唯一ID）。如果消息和队列是可持久化的，那么broker先将消息写入磁盘，然后再发出ACK响应给生产者。

  - 同步confirm：每发送一条消息后，调用waitForConfirms()方法，等待服务器端confirm；由于是同步，效率低下。而且如果MQ服务不稳定，可能会导致业务线程一直阻塞，甚至会拖垮应用
  - 批量confirm：每发送一批消息后，调用waitForConfirms()方法，等待服务器端confirm；当一批数据中的某一条数据发送失败的情况下，就会导致这一批所有的数据都会ACK失败，如果处理不当，还很容易产生消息重复消费的问题。
  - 异步confirm：提供一个回调方法，broker确认了一条或者多条消息后，生产者端会回调这个方法；吞吐量高，不用等待上一条消息返回ACK响应，就可以继续发送下一条消息。实现较上面两种比较复杂。在channel上添加Confirm监听事件，handleNack方法处理消息发送失败的逻辑，handleAck方法处理消息发送成功的逻辑。

<hr style=" height:2px;border:none;border-top:2px dotted #185598;" />

消费者确认机制

消费者收到消息，向RabbitMQ发送ACK回执，告知消息已经被消费。

- 自动ACK：消息一旦被接收，消费者自动发送ACK（在消费者程序抛出异常，消息依然被消费）
- 手动ACK：消息接收后，不会发送ACK，需要手动调用

如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便，如果消息非常重要，不容丢失，使用手动ACK

`basicConsume`方法实现：第二个参数true，自动ACK

```java
channel.basicConsume(SAMPLE_QUEUE_NAME, true, consumer)
```

`basicAck`手动进行ACK

```java
/* 手动进行ACK
* deliveryTag：该消息的index
* multiple：是否批量处理，true:将一次性ack所有消息。
*/
channel.basicAck(envelope.getDeliveryTag(), false);
```

```java
// 监听队列，第二个参数false，手动进行ACK
channel.basicConsume(SAMPLE_QUEUE_NAME, false, consumer)
```

### 消息推拉处理模式

在Rabbitmq中有两种消息处理的模式

- 推模式：消息中间件主动将消息推送给消费者,投递消息的个数还是会受到channel.basicQos的限制,这个模式将消息提前推送到消费者，消费者必须设置一个缓存区来存放消息。缓存区可能会发生溢出，但是此模式实时性比较好。`channel.basicConsume`方法
- 拉模式：消费者主动从消息中间件拉取消息，在消费者需要时才去消息中间件拉取消息，这段网络开销会明显增加消息延迟，降低系统吞吐量。实时性较差，消费者难以获取实时消息，具体什么时候能拿到新消息完全取决于消费者什么时候去拉取消息。

总结

1. 推模式更关注消息的实时性
2.  推模式直接从内存缓冲区中获取消息，能有效的提高消息的处理效率以及吞吐量
3. 拉模式更关注消费者的消费能力，只有消费者主动去拉取消息才会去获取消息
4. 默认使用推消息，由于某些限制，消费者在某个条件成立时才能消费消息的场景需要从批量获取消息的场景

### 死信队列

死信队列也是普通队列，支持队列中的消息是从正常队列中移除的消息，正常队列如果不设置死信队列，这些消息将会被抛弃掉。

- 消息被消费方否定确认，使用 `channel.basicNack` 或 `channel.basicReject` ，并且属性`requeue`设置为false
- 消息在队列中存活的时间超过了队列设置的TTL时间
- 消息队列的消息超过了队列的最大队列长度，会把旧的消息丢弃掉

配置死信队列分为以下步骤：

1. 配置业务队列，绑定到业务交换机上
2. 为业务队列配置死信交换机和路由key
3. 为死信交换机配置死信队列

死信消息的变化

1. 如果队列配置了参数 x-dead-letter-routing-key 的话，“死信”的路由key将会被替换成该参数对应的值。如果没有设置，则保留该消息原有的路由key。例如：如果原有消息的路由key是testA，被发送到业务Exchage中，然后被投递到业务队列QueueA中，如果该队列没有配置参数x-dead-letter-routing-key，则该消息成为死信后，将保留原有的路由keytestA，如果配置了该参数，并且值设置为testB，那么该消息成为死信后，路由key将会被替换为testB，然后被抛到死信交换机中。
2. 由于被抛到了死信交换机，所以消息的Exchange Name也会被替换为死信交换机的名称。



基于死信队列可以实现延时队列

设置队列的TTL时间，超时之后移入死信队列，消费死信队列中的消息

# 联通数科

1. 设计组件通用性方面需要有哪些考虑？
2. 组件开发应用到了设计模式？ 
3. HashMap的了解，1.7和1.8的区别？
4. HashMap怎么解决Hash冲突的？
5. Spring三级缓冲是什么？
6. Spring中AOP的了解？
7. 解释一下Spring AOP中切点，切面？
8. 解释一下AOP 代理，两种代理？有什么区别
9. @Transactional 什么情况下会失效？
10. Redis 用过哪几种数据类型？
11.  Redis 淘汰策略
12. <font color=red> Redis设置内存大小，并且满了，如果要操作，会有什么结果？</font>
13. Redis 是单线程还是多线程？
14. Redis 中持久化有哪几种方式？
15. 持久化AOF，如果一条命令执行了100次，文件中会记录多少次 ？
16. AOF 对相同命令有什么操作吗？
17. Redis主从复制原理
18. Redis 分布式锁
19. 使用分布式锁，如果业务逻辑时间长，并且超过过期时间，怎么处理？怎么做时间续期 
20. 说一下mysql 聚集索引和非聚集索引的理解
21. 解释一下索引下推
22. Mysql事务是怎么实现的
23. MVCC 中的三个参数
24. atomicInteger 实现原理
25. volatile 关键字作用
26. 解释一下内存屏障
27. ConcurrentHashMap 说一下理解
28. 说一下对synchronized的理解
29. 公平锁和非公平锁的实现，哪个方法实现？
30. 说一下spring 序列化和反序列化
31. Mysql 有几千万数据，要加一个字段，要怎么处理？
32. JVM 用过哪些命令，Linux1环境，查看一个java进程内存使用情况，使用哪些命令？
33. ES 用过哪些聚合函数
34. 动态脱敏，静态脱敏































